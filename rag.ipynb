{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69aadbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv() # load from .env file\n",
    "\n",
    "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
    "  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = init_chat_model(\"gemini-2.5-flash-lite\", model_provider=\"google_genai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c292ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding\n",
    "\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")\n",
    "\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "vector_store = InMemoryVectorStore(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50bc0112",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "\n",
    "\n",
    "\n",
    "# Load and chunk contents of the blog\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Index chunks\n",
    "_ = vector_store.add_documents(documents=all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d82bd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # same thing but written slightly differently, and better\n",
    "\n",
    "# from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# urls = [\n",
    "#     \"https://lilianweng.github.io/posts/2024-11-28-reward-hacking/\",\n",
    "#     \"https://lilianweng.github.io/posts/2024-07-07-hallucination/\",\n",
    "#     \"https://lilianweng.github.io/posts/2024-04-12-diffusion-video/\",\n",
    "# ]\n",
    "\n",
    "# docs = [WebBaseLoader(url).load() for url in urls]\n",
    "\n",
    "\n",
    "# from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "# docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "# text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "#     chunk_size=100, chunk_overlap=50\n",
    "# )\n",
    "# doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5dab8fb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_splits[0].page_content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f066ca00",
   "metadata": {},
   "source": [
    "## RAG workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a2dabc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define prompt for question-answering\n",
    "# N.B. for non-US LangSmith endpoints, you may need to specify\n",
    "# api_url=\"https://api.smith.langchain.com\" in hub.pull.\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "\n",
    "# Define state for application\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "\n",
    "# Define application steps\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "\n",
    "# Compile application and test\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3519d4bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'retrieve': {'context': [Document(id='5edda4fa-b1aa-47d6-bf91-10ff288fc2e6', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\nAnother quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#'), Document(id='fc5a2dce-37d6-4805-8718-8ed57418cb07', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Component One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.'), Document(id='fab3cbda-0598-4b3c-97ec-6250777ff300', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Short-Term Memory (STM) or Working Memory: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds.\\n\\n\\nLong-Term Memory (LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM:\\n\\nExplicit / declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts).\\nImplicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.\\n\\n\\n\\n\\n\\nCategorization of human memory.\\n\\nWe can roughly consider the following mappings:'), Document(id='e776e259-0b44-4d21-af23-b2df0743366e', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Conversatin samples:\\n[\\n  {\\n    \"role\": \"system\",')]}}|\n",
      "{'generate': {'answer': 'Task decomposition is a technique used to break down complex or hard tasks into smaller, simpler, and more manageable steps. This process allows models, often through methods like Chain of Thought (CoT), to utilize more computation to tackle intricate problems effectively. It transforms large tasks into multiple manageable sub-tasks, enhancing model performance on complex assignments.'}}|\n"
     ]
    }
   ],
   "source": [
    "for step in graph.stream(\n",
    "    {\"question\": \"What is Task Decomposition?\"}, stream_mode=\"updates\"\n",
    "):\n",
    "    print(step, end=\"|\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf86385",
   "metadata": {},
   "source": [
    "## Agent tool calling RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1dd177ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage\n",
    "from langgraph.graph import MessagesState, StateGraph\n",
    "\n",
    "graph_builder = StateGraph(MessagesState)\n",
    "\n",
    "# allow model to re-write user queries to include context based on past messages\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "\"\"\"\n",
    "1. User inputs as HumanMessage\n",
    "2. query to vector store as AIMessage(with tool calls)\n",
    "3. query result comes back as ToolMessage\n",
    "4. finally generate() responds as AIMessage\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve(query: str):\n",
    "    \"\"\" retrieve information related to a query\"\"\"\n",
    "    retrieved_docs = vector_store.similarity_search(query, k=2)\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"Source: {doc.metadata}\\nContent: {doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    return serialized, retrieved_docs\n",
    "\n",
    "tools_set = [retrieve]\n",
    "llm_with_tools = llm.bind_tools(tools_set)\n",
    "\n",
    "# node 1\n",
    "def query_or_respond(state: MessagesState):\n",
    "    response = llm_with_tools.invoke(state['messages'])\n",
    "    \n",
    "    return {\"messages\" : [response]}\n",
    "\n",
    "# node 2 - node to do tool calls, adds result as a ToolMessage to the state\n",
    "tool_node = ToolNode(tools=tools_set)\n",
    "\n",
    "# node 3\n",
    "def generate(state: MessagesState):\n",
    "    # collect all the retrieved context\n",
    "    recent_tool_msgs = []\n",
    "    for msg in reversed(state['messages']):\n",
    "        if msg.type == 'tool':\n",
    "            recent_tool_msgs.append(msg)\n",
    "        else:\n",
    "            break\n",
    "    tool_msgs = recent_tool_msgs[::-1] # reverse\n",
    "    \n",
    "    context_combined = \"\\n\\n\".join(doc.content for doc in tool_msgs)\n",
    "    system_message_content = (\n",
    "        \"You are an assistant for question-answering tasks. \"\n",
    "        \"Use the following pieces of retrieved context to answer \"\n",
    "        \"the question. If you don't know the answer, say that you \"\n",
    "        \"don't know. Use three sentences maximum and keep the \"\n",
    "        \"answer concise.\"\n",
    "        \"\\n\\n\"\n",
    "        f\"{context_combined}\"\n",
    "    )\n",
    "    \n",
    "    # non tool calls\n",
    "    conversation_msgs = [\n",
    "        msg\n",
    "        for msg in state[\"messages\"]\n",
    "        if msg.type in ('system', 'human')\n",
    "        or msg.type == 'ai' and not msg.tool_calls\n",
    "    ]\n",
    "    past_convo = [SystemMessage(system_message_content)] + conversation_msgs\n",
    "    \n",
    "    response = llm.invoke(past_convo)\n",
    "    return {\"messages\" : [response]}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "98d7a692",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "graph_builder.add_node(query_or_respond)\n",
    "graph_builder.add_node(\"tools\", tool_node)\n",
    "graph_builder.add_node(generate)\n",
    "\n",
    "graph_builder.set_entry_point(\"query_or_respond\")\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"query_or_respond\",\n",
    "    tools_condition,\n",
    "    {END: END, \"tools\": \"tools\"},\n",
    ")\n",
    "graph_builder.add_edge(\"tools\", \"generate\")\n",
    "graph_builder.add_edge(\"generate\", END)\n",
    "\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7a30872d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALcAAAGwCAIAAABkfmPEAAAAAXNSR0IArs4c6QAAIABJREFUeJztnXdcU1f/x092SAh7b0H2RkRF68Jdt9Y6W9vH1latrdVWa2trRatWq3a4WlcdOOosLhA3olVkL2ULREAIkL3z+yP+KI9P4IImOSd43n/4knvvOfdzk0/O93vvPYOk0WgABtMhZNgCMCYAdgmGGOwSDDHYJRhisEswxGCXYIihwhbQWZ7VyIXNCrFAJZeq5RI1bDnEUOkkCoXEsqCwOFRbZ4aZuQn/IEmIPy+pLBSX5QrL8kSeASypRM3mUCztaCol0pq10BlkQbNSLFCJ+UohX8lkUbxD2L5RFhwrCmxpXQZdl1QWitPONzh5Mh08mN4hbDNz0/tw2/K0QlqeK2p4KrO0pcWOtaUxTKlpQdQlV47USUWqfmPt7FzosLXomdw7LWnnG/q9aRc2wBK2ls6CnEt4tfKETU/e+tTd0YMBW4sBSb/S1NwgHzbDEbaQToGWS0TNqrO7a2Z+6UEiwZZieIoeCMoLRKPfdYIthBiEXFJXKb12vH7Glx6whRiPxw8FuWktUz5xgy2EAFRyKKVCc3pHzWtlEQCAXy+OXxTn5qlnsIUQgIpLrhypnfWlJ2wVEAjtb8lkUx6lC2AL6QgkXJKX1mJmTrWwNZlHfPolaqj19ZP1sFV0BBIuuZPYGDvWFrYKaNDopMhB1g+SebCFtAt8l+SmtsSMsKEz4SuBSJ/RNjWlEjWqLx7gfzdF6XwXb6Yxz1hSUjJ27NiXKHj8+PHvvvvOAIoAAIBhRinLFRqo8lcEskskQlVLo9LR06guycvLe7mC+fn5+tbyLz2C2RX5IsPV/ypAfl5SlC5oqpP3e9MgSUlLS8vu3btTU1Obm5uDgoLGjBkzfvz47du379+/X3vAsmXLpk+ffvv27aSkpIyMDIFAEBISMm/evF69egEAEhISDh48uGLFiuXLl0+fPj0vLy87O1tb8NixYz179tSvWoVM8/fv3CmfuOq3Wr0A+baiqU5uuIwkPj6+oaFh5cqVXl5eJ06ciI+P9/b2XrhwoUqlSk5OPn/+PABALBZ//fXXsbGxa9asAQAkJycvWbLk3Llz1tbWdDpdLBYfPHgwPj4+MDBwyZIlc+fO9fT0/P777w2hlsYgNdXJpCI1kw0/DXgByC4R8ZXWDiwDVZ6RkfHee+/17dsXALB48eJhw4bZ2Ni8cAyLxTp27BiLxbKysgIABAQEnD59Ojs7e/DgwRQKRSwWL1iwIDo62kAKXxRjQRXxlUw2ci84YbukRcm2MFSXgIiIiD///LOxsTE6Orpv375BQUG6NYhEv/32W0ZGRkNDg3ZLU1NT6972ShkCtiVVxFfaOiPnEsiNG5lMIlEM9WZv9erVM2fOTEtL++yzz4YNG7Zr1y6lUvnCMU+fPp03b55arV6/fv29e/fu3LnzwgF0uvG+MyqVBDQovueE3JYw2RRRy4vfnL6wsLB4//3333vvvezs7GvXru3Zs8fS0nLGjBltj0lKSlIoFKtXr2YymQCA5uZmA4npDIImBctgLeurANklLAuKiG8QlzQ3NyclJU2cOJHBYERERERERBQWFj569Oh/D7OwsNBaBACQkpJiCDGdRMRXGS7+vgqQI46NI0MpN8itOIVC2blz5/Lly3Nycng83vnz54uKisLDwwEAHh4eDQ0NN2/efPLkiZ+fX0NDw9mzZ5VK5Z07d7KysszNzWtra3XW6e7uXlBQkJ6e3jZx0RdqJbBxpKPZcZOyevVqiKc3Y1NunHoWOdhK7zUzGIywsLDk5OT9+/cfOnSourp6/vz5EyZMIJFIdnZ2BQUFBw4csLa2njZtmlKpTEhI+OWXX/h8/sqVK0Ui0aFDh/h8vo2Nze3bt+fNm0cmP/8tWVtb37p1KyEhITY21sXFRb+CS7KFwmZlz3Bz/VarF+D3Qjq2+cmwGY52rt25/2JnSD5c5xnI8u/FgS1EB/Af4PhHW3DLpLBVwEcqUvUIYsNWoRv4XToiB1v9trQkdIBle31dL1++vGHDBp27lEollar7EuLj49944w19Cm3DsGHD/vemWotGoyG1cyXHjx93dNTdHTrjWpOdC4NuBv9HqxP4EQcAkHG9SSJU9R9np3OvWCxu7wZVIBBwOLqbaBsbm9Y7F73D5XLb2yWTyRgM3dHT0dGRQtGdnP72ecmin3oCFJ+VAFRcAgBI/J07Yo4TA9Ufk0HJvN5MpZNC+6M7PAeVb2Xo2w4JP1bCVgGB4kxhfbUUZYsg5BK2JXXo246nfq2GLcSocEslD67wRs5BfUgOKhFHS1Od4uqJ+qlI9rHQOxUF4oxrvMmLUB+Mg5xLAAA1pZKL+55OW+JuaUeDrcWA5N5pqSgQjftAz4/mDARyLgEAyMTqlKN1DBY5dqwdi4PiE+tXoSRbmJbYENjHsvdwa9haOguKLtFS9EBwJ7EhMMbCyYvpHcxG9i6xk7Q0KMrzRHVVMrVK03+crYWtKbWU6LpEy6N0QUmOsDxPFNrfUqnQsC0olrY0tCU/h0IjiVqUIr5KxFcKmpRSkapHMNsviuPgbnrvIlB3SStVjyWCJoWYr1LI1RKhSr+VP3z40NPT085O92O9l4NuRiaTSCwLCtuSaufCsHYwpcbjBeA/oe8k7n5mAJgZqPKzdy71j5weG2u8zoumBSrPSzAog12CIQa7BEMMdgmGGOwSDDHYJRhisEswxGCXYIjBLsEQg12CIQa7BEMMdgmGGOwSDDHYJRhisEswxGCXYIjBLsEQg12CIQa7BEMMdgmGGOwSDDHYJRhisEswxGCXAO2Eju1Nc4XBLnmOTCYzlTGOUMAuwRCDXYIhBrsEQwx2CYYY7BIMMdglGGKwSzDEYJdgiMEuwRCDXYIhBrsEQwx2CYYY7BIMMdglGGKwSzDEmMzc0YYgKioKANDa/0i75J6Li0tiYiJsaWjxWrclPj4+WpdoIZPJVCp11qxZsHUhx2vtktmzZ5uZ/des5W5ubpMmTYKnCFFea5dMmDDBze3fpa6oVOrkyZPbW8Hzdea1dgkAYNasWa22cHV1nTZtGmxFKPK6u2T8+PHa5oRCoUyaNIlGM+H1SQzH6+4SAMCMGTMYDIaHh8fUqVNha0EU4vVxnlXLGmpkQr7uBdu7Aa7sgb18KoODg3NviwEQw5ZjEBhMCseG6uDGZFu+zDqIHT0vUSk1f//OVco1lg4MJqu7rbL4WkFnkOsqJSQy8AwwCx9o1dXi7bpEqdCc3ckNe8PG2dtQK1xhjM+tU7VeQezgvpwulWo3Lzm3ixs+CFukuzFwilNxpqA8X9SlUrpd8rRMSqGRnbywRbohkUNss242d6mIbpc0cGXmliaz8COmS1g5MLhlki4V0e0SiVBlZo7T1e4JmQKYLIpUpO5CEZ1bNRrwGr8q7v5oNBoN6MIXjJ+qYYjBLsEQg12CIQa7BEMMdgmGGOwSDDHYJRhisEswxGCXYIjBLsEQg12CIQa7xOR5Z+6UX7dvNugpsEswxGCXYIjRW1cjsVi8bv03GRn3VSrVooXLnj6tufdP6v69J/LzcxYtfn/H9j8DA4K1R06fOXbI4BHzP1wMAGhoeLZj55b8ghyZTBYTE/vuOx+6urgBAE6eSjh2/OBnn65Y/f3yyZOmFxTmmptzNvzwc+vpVn6zRCgU/LJtT8eStmz7ISsrXSDge3l6jxkzccL4qQCA4pJHH86ftX7dtk0/xdvZ2u/edbiDSsaNH/ze3I9u3ErJzc26kHiLxWJdvHQu8fzpiopSb2/foUNGTpk8XXtkRUXZgT93Z2alUyiU4KCwt6fNCQkJBwCMfnPAO3M+yC/IuXPnJpvNDg/v9dXyNebm5tpSBw/tSU4+X/+sztHRuVdUzOJPviSTySUljz+YP3PH9j+PJOy7c+emg4Oj9hPTDnyvqCjbsPG7J1UVERHRc2bP08e3R4De2pIt236oKC/9edueYwnnKyrLrl1PolEJRkAplcrPl32Um5e1bOmq/XtPcDgWH38852ktFwBAo9ElEvGx4wdXfhU/fvzUMaMnPHhwt4Xfoi0oEokePLg7csTYjutfsXLx06c169ZuPX70Qv/+g7f9vOFxcREAgE6jAwD27Ns+/e13lixZ2XElNDr99Jljvr4BmzftYDAYV65c3LQ5PsA/6OiRxPfmfnTir0M7dm4FAMjl8s+XfUSj07f+tHvjhl8BAF+v+lwmk2mv5eSphMmTpl+9cn/j+l8ryku37/hJW/n+A7vOnjux4OPPT/6VNPfd+VdSLp45cxwAQKfTAQCbf4ofPmxM8uW7K5Z/f/zEoRs3UwAACoVi+Vef2Ns77t/717z3FyYk7G/iNXbli3oZ9OMSoVB482bKtGlz/HwDbGxsFy1YSqVQCee8yM7JqKqq/GrFmt7Rfa2tbRZ+/Lm5OefUqaPakXZisfg/7y8YOmSEm6v7sLjRdDr96tXL2oKpqdepVOrQISM7qPzeP3dyc7OWf/Gdv1+glZX1O3PmBQWFHj68V1s5AKB/7KC3ps4K8A/qWCSFQrGzd/hk4bJeUTEUCiXxwumwsMhPFy+3srKO7tXn3Xc+PH3mWEtLc1VVZVMTb8rkGd7ePX17+q/+buPq7zYqlUrtnAY+3r5Rkb3JZHJwcNjYsZOvXU9SqVQCoeDosT/ffefD2NiBFhyLuKEjJ06YdujIXrVaTSaTAQCDBw0fNDCORqNFRkQ7Ojo9flwIALh1+1p9fd3CBUsdHZ28vXsuWrhMKBJ2/RvrGvpxyZMn5UqlMjAw5HmlZHJAQDBhb6jc3CwajRYV2bu1VFh4VG5uZusB/n7Pv0I6nT5yxNiUq5e0f96+c33woOEvTBfwAuXlJSwWy8PDq01tgY+LC1v/9PMN7OTVtR6pVCoLCnJ7R/dr3RUZ2VulUuXmZrm5eVhZWa/f8O2RhP35+TkUCiUyIprNZmsP8/Hxay3i6uoul8vr6murqioVCkVQUGjrLl/fgJaWZm1rCgDw8/tXobk5RygUAABqaqqYTKaTk7N2u6Ojk62tXScv5KXRT17C4zUCAFhmrNYtZm3+3x5CoUChUAyJi267se01axteLePGTpn34Yy6ulpzc84//9zZsnlXx5U3Nja8oMHMjCUW/TvCgN7puQVaZUilUpVKtXffjr37drQ9oKmZx2Awft76x4WLZ/86eWTP3u2uru5z350/LG6U9gAGg9l6sPb/IpGQx2sAADDb7NIKlojFTCZT+7P5XzF8fgubbd52C5Np8KEO+nGJpaWV9kNs3SIWtzviQ6VSaf9ja2tnZma2bu3W/xJE0S3Jx8c3wD/o4qWznp7eTk4uoaERHUtis9kvaBCLRbZ29p27IN2Ym5szmcxRI8cNHBjXdrurizsAwMPD6+OPPntv7kfp6fcuJyeu++EbL0/vnj39tJ5oPVgmkwIAzJhm2i9bIv23O7tEIgYA2NnZa5sNnVhYWMplsheu61UuqjPoxyVOTi4AgILCXO2HolQqtXcl2uwPACD9/8+CL+Dz/j/b8vb2lUgkTk4uzk4u2i013Goba9v2zjJmzMRjxw969+g5ZvQEQkn+fkESiaSsrMTbu6d2S0FBbg8vn1e8Um9vX4lUEhnxvP2Ty+V1dU8dHBwrK8sLi/JGjRzHZDIHDBjct++AkaNji0uKtB9IdvbD1hpKSh4xmUwnJxcLSysKhZKXl+3nG6DdVViYZ21tY2Vl3YFLnBydBUJBZWW5p2cPAEBhUX5TE+8VL4oQ/eQl9vYOISHhe/ftqOFW19XVbt22vvXX4+XpzTHnJCWf17pnw8bvOBwL7a4+MbExMbGbNq2pq6ttbm46feb4Rx/N1h6pk7iho3i8hvsP0kYMf5NQUkxMrIuz6+Yta4seFfB4jX/s+e1xcdHUKTNf8Urnf7D41q2rFy+dU6lUOTmZ38evWPrFx3K5vLm5aeOP3+/cta2GW11RUXb4yD61Wh0cFKYt9ayh/uSpBJVKVVlZfv7CmcGDhlOpVAuORVzcqEOH96Sl3RIIBZeTEv9OPEmoMDZ2EJ1O37xlrVQqffasfv2Gb1s/T8Oht+clX61Ys23b+nkfTJdKpXFDR74xYKg2VaTT6atWrf/5l41D4qLt7R0+mv8Zr7Gh9fZn/bptfyeeWrP2q4KCXA8Pr9GjJ0yc8FZ7p2CxWFFRMRqNpjP5GpVKXRu/ZdfubQsWvstgMLy9fdfFb2mbKr4cYWGRu3cePpKwf9eubXKFPCgwdG38FjqdHh4e9fmSlQf+3H3ir8MAgN7Rfbf+tLs1dx43dnJOTub2HVu0uxYuWKrd/snCL3ZStsavW6lUKl1d3efMnvf2tDkdCzA3N1+3duvu3T+PHT+IyWR+NP+zi5fOGXoKRd2jyf+5xFMoQPggm5eu96ct6wqL8vb8fvTV5P0XUql02vQxK1es6dt3gB6rNTQTJsVNmTzjnTnGePzVSY5vLpu1wtOM3dmBeaYxzPNpLZfLrT51+miPHj59+vSHLee1wzRccuXKxf0HdgUHh323akPr9Kz5+TkrvlrcXpGjCedbn4J3gF4q6fYYKuIYh9YHUP9L632TcSoxLbpnxGkPvXyL3dUKegT3HMAQg12CIQa7BEMMdgmGGOwSDDHYJRhisEswxGCXYIjBLsEQo9slTHOyWmV0LRhjQaGSu7SwgG6X2DoxnlV3beJYjKnAq5UxWeT/f2faKXS7xM3XTCZVC3gKvUnDIENxJj9sQNeWtWg3Lxk3zzktsV7Y3G2XxXk9eZjSaMYmB/frWifIjtbHETYrT/5a7ejJsrKnM1k4zzVhqHTysyqpSqGmm5EGTe7yQALiVadLskTPqqVCPtLZrFAorKmp8ff3N/6pRSJRdVW1fwCEU3celjmFbUlxcGe6eDM7cfiLdIe1yWUy2caNG7/99ltYAh48eFBdXd2NFyLuDi7BGBqTzza2bt2amZnZiQMNzoYNGwoKCmCrMAim7ZK///7b398/MjISthAAAFixYsXevXtFIoOPxzQ+OOJgiDHVtqSuru6HH36ArUIHRUVFO3fuhK1Cz5hqWzJr1qyDBw9q56tBjcTExJaWltmzZ8MWojdM1SUYY2J6EefMmTPZ2dmwVRCza9euuro62Cr0g4m1JX/99RePx5s/fz5sIZ1i2LBhKSkpsFXoARNzCQYKJhNxRCLRnj0dze6KJo8fP7548SJsFa+KybhkwoQJU6dOha2iy/j5+VVWVu7btw+2kFcCRxwMMSbQljx8+LCwsLATByLNmTNnTPfhPeouOXv27KVLlwIDOzuDL7KMGDHizTeJJw1EE6Qjjkqlkkgk3WY2ItO9HHTbEpVKlZSUZIqfaXtQKJTGxsa8vDzYQroMui6ZOnVqaOirTryJGp6enomJiadPn4YtpGsgGnHq6urMzc1bZ/vvZtTU1NjZ2TE6PRE+dFBsSyorK2UyWXe1CADA1dU1MzNTuy6KSYCcS65du7Z9+3YPDw/YQgxLjx49Jk6cCFtFZ0Er4shkspKSkuDgYNhCjEFLS8uzZ8969uwJWwgxaLnk0aNHvr6+OteF6ZY0Njaq1Wp7+1daj8UIIPR9HD169OrVq6+PRQAAtra2M2fOlMvlsIUQgNCswC4uLto1pl4rQkJC2i4phiZoRRwMmiDUvHO53LKyMtgqjE1qaipsCcQg5JKbN2+eOXMGtgpjs2TJEtgSiMF5CWQGDDCBFaFwXoIhBqGIU11dXVJSAluFsblx4wZsCcQg5JLbt2+fO3cOtgpj88UXX8CWQAxCeYmbm1vHi9J3SwYPHgxbAjE4L8EQg1DEwXkJsiDkEpyXIAvOSyCD8xJMNwGhiIPzEmRByCU4L0EWhPISd3f37jT6pmMiIyNJJBKJRGr9PwCgX79+27dvhy1NBwi5xCTee+kLFxeX1pmStLPDOTs7L1q0CLYu3SAUcaqqqh4/fgxbhZGIjIx84b4hNDQU2eHQCLkkNTU1MTERtgojMWPGDCcnp9Y/nZ2d58yZA1VRRyDkEnd3dz8/P9gqjERwcHBUVFTrn+Hh4cg2JDgvgcnMmTMzMzNra2udnJymT58OW05HINSWvFZ5CQAgMDAwLCxMm6OEhITAltMRCLUlqampXC536dKlsIXoRqMGT8slTfUKqVhvC0oNCHlHUGXfL3D0w6tN+qqTyaJYO9Cce5iR9NcCIPSEPi0tjcfjjR07FrYQHdRWSm+fbQAAuPiwlTI1bDkdQaWTuWUiAMAbE+2cPPXTjxghlyBLfbX85qn6YTNdqfSuLK8KFaVck5JQM2iKvYObHua/QCgvqaysLCoqgq3iRWQS9dkd1aPmupmQRQAAVDpp1Fy3sztqZBI9tHwIuSQtLe3ChQuwVbzIw5SmXnF2sFW8JFFxtukpesh4EHKJp6cnlEU8O6b2idTClgZbxUtiaUuveyJ99XoQuseJjY2FLUEHMpGKZYHQp9QlWBZUqUgPd2QItSVo5iVqtUajNtUEX6MBapUexCP0K0lLS+NyuQEBAbCFYF4EIZd4enpyOBzYKjA6QMglaOYlGLTykoqKim6wKkG3BKG25O7du1wuF+UX6K8tCLnEy8vL0tIStgqMDhBySb9+/WBLwOgG5yUYYhBqS3BegiwIuQTnJciCkEtwXoIsCOUl5eXlprgOld4pKysZEhedm5sFW8i/IOSSe/fuJSUlwVahB1Z/v/zipW414Bkhl/To0aN7rHlS9CgftgQ9g1Be0rdvX9gSXhWNRjN0WG8AwKbN8bt//+XcmasAgIOH9iQnn69/Vufo6NwrKmbxJ1+2rtvRwa7WCk+eSkhOvlBd88TTo0evXn3ef+9j7bhiY4JQW9IN8hISiXT54h0AwBfLVmktsv/ArrPnTiz4+POTfyXNfXf+lZSLZ84c1x7cwa5WTp8+lnD0wFtTZx05dG7MmInnL5z56+QR418XQm3JvXv3uFwu4uOXuoRAKDh67M+FC5bGxg4EAMQNHVlWVnzoyN5Jk94WiUXt7WpbQ3ZORkBA8IgRbwIAxo+bEhUVI5PqoYdiV0HIJd7e3jY2NrBV6JOqqkqFQhEU9O96t76+AS0tzU9ruS0tze3taltDSEj473/8+uOmNeFhUbH9B7m5uhv3Cp6DkEv69OkDW4Ke4fEaAABMxr9Dp8zMWAAAiVjcwa62qcmUyTPMzFhpd29t+HE1lUodOnTkh/M+sbU1dp9+hFxSXl4uEom6U8Rhs80BABKppHWLRCIGANjZ2QuE/PZ28XiNrRspFMq4sZPHjZ1cXl6akXH/wJ+7xSJR/JrNRr4QhLLXbvO8pBUfHz8KhZKXl926pbAwz9raxsrKuoNdrVs0Gk1S0vmKijIAQI8ePlOmzJg8eXpJySOjXwdKLvHx8ekGS9YzGAx7e4eMjPuZWeksM1Zc3KhDh/ekpd0SCAWXkxL/Tjw5dcpMAIAFx6K9Xa2QSKSk5PPfff/l3bu3+QL+vXupqXduBIeEG/+iEIo4MTExsCXoh1kz399/YNe9f1JPHLv0ycIvdlK2xq9bqVQqXV3d58ye9/a053MedbCrleVfrv5t++aV3ywBANja2o19c9JbU2cb/4oQGk1eWloqFotRa06ObKgcNNXZ0h71ZTp10tKguHGCO/srz1esB6GIc//+/eTkZNgqMDpAKOL4+PjY2trCVoHRAUIu6TZ5SfcDoYhTWlqam5sLWwVGBwi5BOclyIJQxPH19bW3t4etAqMDhFwSHR0NWwJGNwhFnJKSkuzs7E4ciDE2CLnkwYMHKSkpsFVgdIBQxMF5CbIg5BKclyALQhEH5yXIgpBLcF6CLAhFHDTzEnNrmkKOymvzrqKQq/UyWS1CLkEzL7G0pTVypXauepjN3fg01kgtbPTgEoQiTnFxcVYWQoNjtYT0syzNFcBW8ZKU5QpC+lm8ej0IuSQ9Pf3q1auwVbyInSs9aojVjRO1sIV0mRt/1UYOsdJLK4hQxPH19XVwcICtQge+EeZKuSblCNfciuboaaZGeyppMplUVykRNisCojm+EfpZnxmhHo2Iw+cpKwtFfJ5S1KLUY7VZWdkREfrs8My2oFrYUr0C2RwbvTUBCLmkuLhYJBJFRETAFmJUevfu/eDBA9gqCMB5CYYYnJdgiEHIJWg+L8GgFXEePXr08OFD2CowOkDIJRkZGTdu3ICtAqMDhCKOv7+/k5MTbBUYHSDkkqioKNgSMLpBKOLgvARZEHIJzkuQBaGIg/MSZEHIJTgvQRaEIg7OS5AFIZfgvARZEIo4gYGBLi4usFVgdICQS163PgMmBEIRp6ioKD09HbYKjA4QcklmZubNmzdhq8DoAKGIg/MSZEHIJTgvQRaEIg7OS5AFIZfgvARZEIo4QUFBbm5usFVgdICQS8LDIczDj+kMCEWcgoKC+/fvw1ZhbKytrTtxFGQQckl2dvbt27dhqzA2TU1NsCUQg1DEwXkJsiDkEpyXIAtCEef1zEtMAoRc8nrmJSYBQhEnODjY3R3OermYjkHIJWFhYbAlYHSDUMTJz8+/d+8ebBUYHSDkkpycnDt37sBWgdEBQhEH5yXIgpBLcF6CLAhFHJyXIAtCLsF5CbIgFHFwXoIsCLkE5yXIglDEycvLS0tLg60CowOE2pLc3FwulxsbGwtbiDGIjIykUChqtZpMJkdFRZHJZLVaHRsb+9tvv8GWpgOEXBIaGurp6QlbhZFwdnaur68nk8kAAO2/Li4uCxYsgK1LNwhFnJCQkNekIdFO1qJWq9tuCQsLCwoKgqeoIxByyWuVl8yYMaPtQEYnJ6fZs2dDVdQRCLkkNzf37t27sFUYieDg4LZjGSMiIpBtSNBySWhoaL9+/WCrMB4zZszQziPn5OQ0ffp02HI6AqHsNSQkBLYEoxIcHBwWFlZbWxsZGYn4tSPkkpycHD6fP2DAAGOelFcrb+DK9LswUud5I3ROc4VVbNC4zOtwxluwLal2LgwbJ3rHhyG0itLRo0e5XO7SpUtcMnTZAAASJ0lEQVSNczqNBpzf81TEV1rY0s3YCP1ajIlEpBTwFGwLypv/cSaR2j0MIZfk5+cLBIK+ffsa4VxqNTjzW01gXyt3f7YRToc4T4pEhfebJy90JbeTpiLkEmNybjfXP9rKtScLthBUqCkRP0pvnjBf9yxDCN3j5OTkpKamGuFEtRUyjRpgi7TFtSdLowa1lTKdexFySX5+/j///GOEEzXWylic1zQR6QAWh9r4VLdLEPqwwsLCevToYYQTiflKlgVCF44IbEuquEWhcxdCH1ZwcLBxTqTRgNcyGSNArQYaoPs+B6GIY7S8BNNVEHKJ0fISTFdBKOKEh4f7+PjAVoHRAUIuQfml6GsOQhEHz0yBLAi5BM9ygywIRRyclyALQi7BeQmyIBRxcF6CLAi5BOclyIJQxImMjPT19YWtAqMDhFwSEBAAWwJGNwhFnKysLLzySZdY/f3yi5fOGeFECLmksLAQr6LUJYoe5RvnRAhFHJTzksbGho0/rs4vyPHw6DFpwrTyitL7D9L2/nEMANDQ8GzHzi35BTkymSwmJvbddz50dXEDAJSUPP5g/swd2/88krDvzp2bDg6OQwaPmP/hYhKJ1EGpk6cSjh0/+NmnK1Z/v3zypOkLPl5y9+7ta9eTsnMyhEJBYEDInNnzIiJ6KZXK4SP7AgA2bY7f/fsv585cBQBcvHQu8fzpiopSb2/foUNGTpmstzE+CLUlAQEB0dHRsFXo5sdN31dVVf60edea1ZtS79x4+PAf7ZetVCo/X/ZRbl7WsqWr9u89weFYfPzxnKe1XAAAnU4HAGz+KX74sDHJl++uWP798ROHbtxM6bgUjUaXSMTHjh9c+VX8+PFTxWLx2h++ViqVX61Ys27tVldX969XLWlubqJSqZcv3gEAfLFsldYiV65c3LQ5PsA/6OiRxPfmfnTir0M7dm7V1+Uj5BJk85LGxob7D+5On/5ugH+Qvb3D0s+/5j6t1u7Kzsmoqqr8asWa3tF9ra1tFn78ubk559Spo60zCQweNHzQwDgajRYZEe3o6PT4cWHHpSgUilgs/s/7C4YOGeHm6s5isfb8ceyzT1dERkRHRkR/+MFisVicl5f9vyITL5wOC4v8dPFyKyvr6F593n3nw9NnjvEFfL18Agi5pLKysri4GLYKHZRXlAIAQkOeD+u1tLSKiHje5uXmZtFotKjI3to/yWRyWHhUbm5ma1k/v8DW/5ubc4RCQWdK+fv9+xhaLBL98uuPU6eNGhIXPW7CYABAc8uLQ7yUSmVBQW7v6H/Hz0ZG9lapVFpTvjoI5SX+/v6urq6wVehAJBICAJhmZq1bLDiWtbVcAIBQKFAoFEPi/itQ2tratf6frGuIC2EpbbQCANTWPv10ybze0f2+/WZ9UFCoSqUaNab//1YolUpVKtXefTv27tvRdntLS/NLXfGLIOQSZJ+XMOgMAIBK+e8o0aZmnvY/trZ2ZmZm69b+VwZApRB8qp0vde16kkKhWP7laiaT2cG3bm5uzmQyR40cN3BgXNvtHu5enbg+YhBySUZGRktLy5AhQ2ALeREXFzdt3HF39wQA8AX8rKx0V1d3AIC3t69EInFycnF2ej7eqYZbbWNt23GFnS/V0tLM4VhoLQIA0Ca/7dYplUT+fyiUy+V1dU/btk+vAkJ5yaNHjzIyMmCr0IGHh5e7u+eBP3dzn9YIhIJt29ZrfQMA6BMTGxMTu2nTmrq62ubmptNnjn/00eyk5PMdV9j5Uj19/BobGy5cPKtUKu/9cycvL8ucbV5fXwsAYDAY9vYOGRn3M7PSlUrl/A8W37p19eKlcyqVKicn8/v4FUu/+Fih0D1yoqsg1JZERUX5+fnBVqGb5V98t+mn+NlzJvr29B8x/E0Wi11a+li7a/26bX8nnlqz9quCglwPD6/RoydMnPAWYYWdLDVs2OjKJ+X7D+za/NPamJjY5V98dzhh36HDe0Vi0ScLl82a+f7+A7vu/ZN64tilsLDI3TsPH0nYv2vXNrlCHhQYujZ+C41G08vlv47jhO8n8WRSEDHYpvNFWlqapVKpo6OT9s8vly9is82/+3aDwTRCIOsGj8EEMSN1fCwIRZyMjIzr16/DVqGbVd8t+3zp/NTUG01NvD8P/pGZlT527GTYoowHQi5BNi8BAKxZvcmrh8+u33+eOXv83bu31qze1CsqBrYo44Hzkk5hZWW9Ln4LbBXQQMgl/v7+sCVgdINQxElPT7969SpsFRgdIOSS4uLirKws2CowOkAo4kRHR4tEItgqMDpAyCXIdkHCIBRxcF6CLAi5BOclyIJQxMF5CbIg5BKclyALQhEH5yXIgpBLjJaXmLGpavVr9yacEI0amJlTdO5CKOL07t3bOHmJjTPtUYbACCcyLeqrJD5huntTIOSSnj17GudErt5mSoVa0KTgWOunk043QNCkUCrUrt5mOvciFHHS09NTUtrt16lPSODN953T/q6XCFTGOB3yiPnKtL/r33zfuZ1JgVFqS4qLi7lc7rBhw4xwLo41deQcx7+2PXHzM7eyozHbicfdHqlQ1dwor34seutTd451u2ZAqEdjSUmJSCQKDw835kkfpQueVcuEkNbaAgDk5eaFhEJbjo1tSXVwY/hHczo+DCGXvJ707t37wYMHsFUQ8FrmJZgugpBLiouLs7N1jJPGQAeh7DUmJkYsFsNWgdEBQi7BUwIjC0IR5/79+8nJybBVYHSAkEtKS0tzc3Nhq8DoAKGIg/MSZEHIJTgvQRaEIg7OS5AFIZfgvARZEIo4OC9BFoRcgvMSZEEo4ty7d+/y5cuwVWB0gJBLysvL8/ONNLE6pksgFHH69u2Lx+OgCUIu6dGjB2wJGN0gFHFwXoIsCLkE5yXIglDEwXkJsiDkEpyXIAtCEefu3bsXL16ErQKjA4RcUlFRUVion/VcMPoFoYjTr18/iUQCWwVGBwi5xMtLP4u5mBApKSkTJ06ErYIYhCIOAKCxsXHUqFGwVRiJ5OTk69evf/3117CFEIPc2D6RSHT16tXx48fDFmJYbty4cf78+c2bN8MW0imQc4l2rcKmpiZ7e3vYQgxFWlrasWPHfvnlF9hCOgtaEUcLlUrl8XizZ8+GLcQgpKenHzx40IQsgmhbooXP55eWlkZGRsIWok9ycnK2bdu2b98+2EK6BrouAQAIBILq6urAwMBOHGsCPHr0KD4+/vDhw7CFdBkUI04rHA6npaVl0aJFsIXogfLy8m+++cYULYJ6W6JFJBIJBAInJyfYQl6empqaBQsWnDt3DraQlwTptkQLm81WqVTp6emwhbwkDQ0N//nPf0zXIqbhEgCAq6trbW3t6tWrYQvpMnw+f9q0aabeu8oEIk4rKpVKpVLR6XTYQjqLTCaLi4tLTU2FLeRVMY22RAuFQikqKkpLS4MtpFNoNJoBAwZ0A4uYmEsAAGFhYTU1NXv27IEthJjY2FhTMTQhphRxTIjBgwdfuHCBzWbDFqIfTKwtaSU5OfnGjRuwVehmxIgRp0+f7jYWMWGXjBgxora2NikpCbaQFxk7duzBgwdtbHRP+2+qaLoRQ4YMMfIZlyxZ8sYbb7T+OXny5IqKCiNrMAKm2pa0smfPHm2SGBkZ2dzcvHHjRqOduqGhobKyUiwWDxkyBAAwY8aMDRs2eHp6Gk2A0TB5l8ybN6+mpqZXr14UCgUAkJGRYbRTZ2Rk1NXVad9KRkdHf/PNN911UTmTd8nbb7+9ceNGEokEACCTyTwez2gLiV6/fr1td+5PPvnEOOc1PqbtkilTppSWlrbdwuPxrl27ZoRTi0SioqIirTu18Pn8uLg4I5za+Ji2Szgcjr29vUajUamer4dEIpGM814wIyOjsbGx9U+1Wk2hUKhUhMYk6BHTvqoDBw6UlZXdvHnz2rVrPB6vtrZWrVY3NjYWFhYauu/S1atXhUIhAIDJZNrY2Pj5+Y0ePXr48OEGPSksTOzZq0yiFvAUIr5KxFfKZeq2u2pra8vLyx8/fiwUCkNDQwcOHGhQJX/88QeJRLKxsQkICPDy8mKxWK27aHQynUFmWVDYFjQre9P+HWoxDZc0NyhLsoQl2UKlEsilaiqDQqFRyRREwyWFRpaL5Sq5SqMBMpHcI4DtH2XuHWrCj2JRd4lEqLp+soHfpNaQqRb2LLY1E7airqFSqPnPxMIGkUKq6D3MOrS/BWxFLwPSLkn9m5d/t9mhp621izlsLa+KSqGuK+ZJhdI333N29DCZLjJa0HXJiZ9raGxza1eT90db5GIFt/BZdJxlSD9TalSQdIkG/P51mUuQg7mt7kWQTR1uwbOwWHZIP4KVN9EBRZfsWVXhEeFMZ3WHu4P24BY86xnKiBlhDVtIp0DuNuH4lmqXIPvubREAgEuQfXG2pCRHCFtIp0DLJbfONJpZW7CsTOxG5uVwDXFMvypoaVDAFkIMQi5pqlc8zhJaOJnwc4WuwrbjpBx7BlsFMQi55ObpBgfv7tXFiwiOnZmQr64pRX2eMFRcUl8lk8tJFg6sThzbrXDwsc28wYetggBUXFKULiDT0H3WlJGTtGxVH7FY/1+nmQWdWyYWtaj0XrMeQcUlZbkizuvXkGjh2LHK85G+2UHCJbw6BY1JZbBosIXAwcLBvOqxDLaKjkDisURzvVzTptOX3imrzLpyfU9VTaEFxy7Qr/+IoR8w6GYAgAMJX1IotMiwEcdPx8vlEk+PsLEjF3m4BWtLnb/8a3r2RQadFRk20s7GzXDyaGbU6mKkFyxEoi0RC5QUKsVAldc9q9jz56cqpXLxh/vmTFtXwy3avX+hWq0GAFCp9IonOZk5yUsWHPzh25sUCuX4mbXaUmn3T6XdPzn5zS8+nb/f2sop5aYB57ii0ilSEc5LiBC1qCg0Q7kkMzuJQqG9O2ODg72ns1PPqRNWPqnOL3h0GwBAIpHlcsm0iV/bWLtQKNSIkOF19WVyuRQAkHr3RFhwXFjIUBbLok+v8T5eUQaSBwAgU0hkCkkmUXfiWDgg4RK1GlCohlJS8STb3S2IzbbS/mln62Zt5VxWkan908Hei8F4njWbmVkAAKQyoUajaeBVOTr8u8aGm6th+0cy2FSV0qBneCWQyEtYHLJCJjdQ5RKpsObpo2Wr+rTdKBA879hMIulwp1QmUqtVTOa/nRboNEO+NNAAUZOcxUHiF6sTNFxiQVUpDPX8kcOx7UGPGDn0w7Yb2SzLDoowGWwymaJU/nvfIZMbMLtUyFVMtqECrl5AwiUcKxqNYahfkouTb1buFZ8eUa1jZ2rry+xtPTooQiKRrK2cK57kvtFvunZL4eM7BpIHAFDKVE5eSD8rQqKVc/JiNHFFKoVB0rdB/WepVMpzF7fK5dK6ZxXnL//6028z656Vd1wqPGRYdl5KTt41AMDVmweqagy4cA+/XmTvivSzIiRcAgDwDDTn1xtk0T42y3LZogQ6jbllx+xNv7xdVpk5bdIqFyeCAb3DBr3XO3Ls6Qublq3q87j0n7EjPgEAaIBBemyJeKKe4Uh33ESlr1pFvvj+daGDjy1sIcZGKVU1VTW8tdgFtpCOQKUt8QpmyfhSqcBQdzrIUl/OC45BuiFBJXvV8sZE29uJPPcw3XNENzRWbds1V+cuMomi1uh+dhkbM2XM8AX6UljxJGfPoSU6d6lUSgqZAnS9Z+jTa8K4UYt1lpKJFHKRLKgv6tNioxJxtJzfV0dhWZhZ6uhCoFarZTLdiYtcLqXTdT/PoFBo7e16OSQSQVeLdKChoawxegjbKwj17nlouQQAsH1ZSdAQLxLZgC//EOFZebODExg02QRSMVTyklZmfulReq8atgqDw6sRUIDMJCyCYlsCABC2qI5uru7Zz82QvQlgwqsWMGmKMXNNZsk55NoSAIC5JWXKQueCq+Xd8pbnWSmPzZSZkEUQbUtaubCvtoWnsfO2oZshdC/20jQ/FdaX8mKG20QM7ugtEoIg7RIAQHGm8NbZBgtHDtOczrFH+mVHe8glSsEzsbBBZO9KHTTJnm2J9Is9naDuEi1F94UFD/jcUrGdp4VGA2gMKpVBQXaWGxKZpJAqlTKlSqkWN0uARtMjiB0+0MrWGemXNR1gGi55jgZUFIp5dXJBk1LUonphxix0YFtQNUBjbkm1tqc6uDNtndEdQdJJTMolGEgg2mhjkAK7BEMMdgmGGOwSDDHYJRhisEswxGCXYIj5P09qTYsTQ/wOAAAAAElFTkSuQmCC",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2e0e949b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "what is self-reflection?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  retrieve (13b8a56d-99a3-4100-8cac-848c73e3201a)\n",
      " Call ID: 13b8a56d-99a3-4100-8cac-848c73e3201a\n",
      "  Args:\n",
      "    query: what is self-reflection?\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retrieve\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "Content: Self-Reflection#\n",
      "Self-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.\n",
      "ReAct (Yao et al. 2023) integrates reasoning and acting within LLM by extending the action space to be a combination of task-specific discrete actions and the language space. The former enables LLM to interact with the environment (e.g. use Wikipedia search API), while the latter prompting LLM to generate reasoning traces in natural language.\n",
      "The ReAct prompt template incorporates explicit steps for LLM to think, roughly formatted as:\n",
      "Thought: ...\n",
      "Action: ...\n",
      "Observation: ...\n",
      "... (Repeated many times)\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "Content: Short-Term Memory (STM) or Working Memory: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds.\n",
      "\n",
      "\n",
      "Long-Term Memory (LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM:\n",
      "\n",
      "Explicit / declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts).\n",
      "Implicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Categorization of human memory.\n",
      "\n",
      "We can roughly consider the following mappings:\n"
     ]
    },
    {
     "ename": "ChatGoogleGenerativeAIError",
     "evalue": "Invalid argument provided to Gemini: 400 * GenerateContentRequest.contents: contents is not specified\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidArgument\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ml-fun/llm/chat-llm/.venv/lib/python3.11/site-packages/langchain_google_genai/chat_models.py:206\u001b[39m, in \u001b[36m_chat_with_retry.<locals>._chat_with_retry\u001b[39m\u001b[34m(**kwargs)\u001b[39m\n\u001b[32m    205\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgeneration_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[38;5;66;03m# Do not retry for these errors.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ml-fun/llm/chat-llm/.venv/lib/python3.11/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py:868\u001b[39m, in \u001b[36mGenerativeServiceClient.generate_content\u001b[39m\u001b[34m(self, request, model, contents, retry, timeout, metadata)\u001b[39m\n\u001b[32m    867\u001b[39m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m868\u001b[39m response = \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    869\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    871\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    873\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ml-fun/llm/chat-llm/.venv/lib/python3.11/site-packages/google/api_core/gapic_v1/method.py:131\u001b[39m, in \u001b[36m_GapicCallable.__call__\u001b[39m\u001b[34m(self, timeout, retry, compression, *args, **kwargs)\u001b[39m\n\u001b[32m    129\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mcompression\u001b[39m\u001b[33m\"\u001b[39m] = compression\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ml-fun/llm/chat-llm/.venv/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py:294\u001b[39m, in \u001b[36mRetry.__call__.<locals>.retry_wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    291\u001b[39m sleep_generator = exponential_sleep_generator(\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m._initial, \u001b[38;5;28mself\u001b[39m._maximum, multiplier=\u001b[38;5;28mself\u001b[39m._multiplier\n\u001b[32m    293\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ml-fun/llm/chat-llm/.venv/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py:156\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    155\u001b[39m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     next_sleep = \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m        \u001b[49m\u001b[43msleep_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ml-fun/llm/chat-llm/.venv/lib/python3.11/site-packages/google/api_core/retry/retry_base.py:214\u001b[39m, in \u001b[36m_retry_error_helper\u001b[39m\u001b[34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[39m\n\u001b[32m    209\u001b[39m     final_exc, source_exc = exc_factory_fn(\n\u001b[32m    210\u001b[39m         error_list,\n\u001b[32m    211\u001b[39m         RetryFailureReason.NON_RETRYABLE_ERROR,\n\u001b[32m    212\u001b[39m         original_timeout,\n\u001b[32m    213\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msource_exc\u001b[39;00m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ml-fun/llm/chat-llm/.venv/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py:147\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     result = \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m inspect.isawaitable(result):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ml-fun/llm/chat-llm/.venv/lib/python3.11/site-packages/google/api_core/timeout.py:130\u001b[39m, in \u001b[36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m] = remaining_timeout\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ml-fun/llm/chat-llm/.venv/lib/python3.11/site-packages/google/api_core/grpc_helpers.py:78\u001b[39m, in \u001b[36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m grpc.RpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions.from_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mInvalidArgument\u001b[39m: 400 * GenerateContentRequest.contents: contents is not specified\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mChatGoogleGenerativeAIError\u001b[39m               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmessages\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HumanMessage\n\u001b[32m      4\u001b[39m input_message = \u001b[33m\"\u001b[39m\u001b[33mwhat is self-reflection?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mHumanMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_message\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpretty_print\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ml-fun/llm/chat-llm/.venv/lib/python3.11/site-packages/langgraph/pregel/main.py:2642\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2640\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2641\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2642\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2643\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2644\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2645\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2646\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2647\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2650\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmpty\u001b[49m\n\u001b[32m   2651\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2652\u001b[39m loop.after_tick()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ml-fun/llm/chat-llm/.venv/lib/python3.11/site-packages/langgraph/pregel/_runner.py:162\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    160\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ml-fun/llm/chat-llm/.venv/lib/python3.11/site-packages/langgraph/pregel/_retry.py:42\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     40\u001b[39m     task.writes.clear()\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     44\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ml-fun/llm/chat-llm/.venv/lib/python3.11/site-packages/langgraph/_internal/_runnable.py:657\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    655\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    656\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m657\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    659\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ml-fun/llm/chat-llm/.venv/lib/python3.11/site-packages/langgraph/_internal/_runnable.py:401\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    399\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    400\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    403\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 72\u001b[39m, in \u001b[36mgenerate\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m     64\u001b[39m conversation_msgs = [\n\u001b[32m     65\u001b[39m     msg\n\u001b[32m     66\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m msg \u001b[38;5;129;01min\u001b[39;00m state[\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     67\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m msg.type \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m'\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     68\u001b[39m     \u001b[38;5;129;01mor\u001b[39;00m msg.type == \u001b[33m'\u001b[39m\u001b[33mai\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m msg.tool_calls\n\u001b[32m     69\u001b[39m ]\n\u001b[32m     70\u001b[39m past_convo = [SystemMessage(system_message_content)] + conversation_msgs\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m response = \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpast_convo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m : [response]}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ml-fun/llm/chat-llm/.venv/lib/python3.11/site-packages/langchain_google_genai/chat_models.py:1334\u001b[39m, in \u001b[36mChatGoogleGenerativeAI.invoke\u001b[39m\u001b[34m(self, input, config, code_execution, stop, **kwargs)\u001b[39m\n\u001b[32m   1329\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1330\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1331\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mTools are already defined.code_execution tool can\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt be defined\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1332\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1334\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ml-fun/llm/chat-llm/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:395\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    383\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    384\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    385\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    390\u001b[39m     **kwargs: Any,\n\u001b[32m    391\u001b[39m ) -> BaseMessage:\n\u001b[32m    392\u001b[39m     config = ensure_config(config)\n\u001b[32m    393\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    394\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    405\u001b[39m     ).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ml-fun/llm/chat-llm/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:980\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    971\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    972\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    973\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    977\u001b[39m     **kwargs: Any,\n\u001b[32m    978\u001b[39m ) -> LLMResult:\n\u001b[32m    979\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m980\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ml-fun/llm/chat-llm/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:799\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    796\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    797\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    798\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m799\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    801\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    803\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    804\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    805\u001b[39m         )\n\u001b[32m    806\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    807\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ml-fun/llm/chat-llm/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:1045\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1043\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1044\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1045\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1046\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1047\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1048\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1049\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ml-fun/llm/chat-llm/.venv/lib/python3.11/site-packages/langchain_google_genai/chat_models.py:1441\u001b[39m, in \u001b[36mChatGoogleGenerativeAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\u001b[39m\n\u001b[32m   1414\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_generate\u001b[39m(\n\u001b[32m   1415\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1416\u001b[39m     messages: List[BaseMessage],\n\u001b[32m   (...)\u001b[39m\u001b[32m   1427\u001b[39m     **kwargs: Any,\n\u001b[32m   1428\u001b[39m ) -> ChatResult:\n\u001b[32m   1429\u001b[39m     request = \u001b[38;5;28mself\u001b[39m._prepare_request(\n\u001b[32m   1430\u001b[39m         messages,\n\u001b[32m   1431\u001b[39m         stop=stop,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1439\u001b[39m         **kwargs,\n\u001b[32m   1440\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1441\u001b[39m     response: GenerateContentResponse = \u001b[43m_chat_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1442\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1443\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1444\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_method\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1445\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdefault_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1446\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1447\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _response_to_result(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ml-fun/llm/chat-llm/.venv/lib/python3.11/site-packages/langchain_google_genai/chat_models.py:231\u001b[39m, in \u001b[36m_chat_with_retry\u001b[39m\u001b[34m(generation_method, **kwargs)\u001b[39m\n\u001b[32m    222\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    224\u001b[39m params = (\n\u001b[32m    225\u001b[39m     {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m _allowed_params_prediction_service}\n\u001b[32m    226\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (request := kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m   (...)\u001b[39m\u001b[32m    229\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m kwargs\n\u001b[32m    230\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m231\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_chat_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ml-fun/llm/chat-llm/.venv/lib/python3.11/site-packages/tenacity/__init__.py:338\u001b[39m, in \u001b[36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[39m\u001b[34m(*args, **kw)\u001b[39m\n\u001b[32m    336\u001b[39m copy = \u001b[38;5;28mself\u001b[39m.copy()\n\u001b[32m    337\u001b[39m wrapped_f.statistics = copy.statistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ml-fun/llm/chat-llm/.venv/lib/python3.11/site-packages/tenacity/__init__.py:477\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    475\u001b[39m retry_state = RetryCallState(retry_object=\u001b[38;5;28mself\u001b[39m, fn=fn, args=args, kwargs=kwargs)\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m     do = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ml-fun/llm/chat-llm/.venv/lib/python3.11/site-packages/tenacity/__init__.py:378\u001b[39m, in \u001b[36mBaseRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    376\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m     result = \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ml-fun/llm/chat-llm/.venv/lib/python3.11/site-packages/tenacity/__init__.py:400\u001b[39m, in \u001b[36mBaseRetrying._post_retry_check_actions.<locals>.<lambda>\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    398\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_post_retry_check_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m, retry_state: \u001b[33m\"\u001b[39m\u001b[33mRetryCallState\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    399\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.iter_state.is_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.retry_run_result):\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m         \u001b[38;5;28mself\u001b[39m._add_action_func(\u001b[38;5;28;01mlambda\u001b[39;00m rs: \u001b[43mrs\u001b[49m\u001b[43m.\u001b[49m\u001b[43moutcome\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    401\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    403\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.after \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.11.13-linux-aarch64-gnu/lib/python3.11/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.11.13-linux-aarch64-gnu/lib/python3.11/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ml-fun/llm/chat-llm/.venv/lib/python3.11/site-packages/tenacity/__init__.py:480\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    478\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m         result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[32m    482\u001b[39m         retry_state.set_exception(sys.exc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ml-fun/llm/chat-llm/.venv/lib/python3.11/site-packages/langchain_google_genai/chat_models.py:218\u001b[39m, in \u001b[36m_chat_with_retry.<locals>._chat_with_retry\u001b[39m\u001b[34m(**kwargs)\u001b[39m\n\u001b[32m    215\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_msg)\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m google.api_core.exceptions.InvalidArgument \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ChatGoogleGenerativeAIError(\n\u001b[32m    219\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid argument provided to Gemini: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    220\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    222\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[31mChatGoogleGenerativeAIError\u001b[39m: Invalid argument provided to Gemini: 400 * GenerateContentRequest.contents: contents is not specified\n",
      "During task with name 'generate' and id '829060b7-444b-61ba-1325-48797da2f783'"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "\n",
    "input_message = \"what is self-reflection?\"\n",
    "\n",
    "for step in graph.stream(\n",
    "    {\"messages\": [HumanMessage(content=input_message)]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
